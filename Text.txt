Full-Text search of Early Music Prints Online (F-TEMPO)

We shall build a publicly-accessible, online resource offering, for the first time, flexible content-based, full-text searching of a large and growing collection of digital images of renaissance and early baroque music distributed amongst the world's music libraries. As well as an easy-to-use online interface, we provide an API allowing flexible and extensible uses of the resource in an indefinite number of ways. For example, bibliographical metadata and search results can easily be shared and linked online with other, non-musical data resources, such as Wikipedia and MusicBrainz. This will open the musical and other information contained within the resource to be exposed as useable knowledge by digital humanists and others. As the resource grows over time to approach a million pages, this will offer a unique perspective on the musical, social, commercial and political history of the early modern period accessible to scholars for the first time.

Abstract
Please provide a short abstract summarising your proposed research in terms suitable for an informed general audience, not one specialised in your field:
We shall build a publicly-accessible, online resource offering, for the first time, flexible content-based, full-text searching of a large and growing collection of digital images of renaissance and early baroque music distributed amongst the world's music libraries. As well as an easy-to-use online interface, we provide an API allowing flexible and extensible uses of the resource in an indefinite number of ways. For example, bibliographical metadata and search results can easily be shared and linked online with other, non-musical data resources, such as Wikipedia and MusicBrainz. This will open the musical and other information contained within the resource to be exposed as useable knowledge by digital humanists and others. As the resource grows over time to approach a million pages, this will offer a unique perspective on the musical, social, commercial and political history of the early modern period accessible to scholars for the first time.

Objectives
We seek funding for programming to develop from our existing proof-of-concept interface, F-TEMPO, a robust system for full-text, content-based searching of a large
‘virtual collection’ of page-images of printed music of the 16th and 17th centuries. With the aid of the British Library and international music libraries, we will add already- digitised images to greatly expand the ‘Early Music Online’ resource (EMO) [1]. EMO originated in a JISC Rapid Digitisation project in 2011; about 300 books of typeset music printed before 1600 were then catalogued and their page-images digitised from microfilms and are now freely available to the public. Access to the collection is provided by standard bibliographical metadata search; there is no facility for content-based exploration. At Goldsmiths, within the AHRC Transforming Musicology project, we built an efficient prototype easy-to-use search interface offering rapid and accurate location of pages bearing similar music to a query [2].
Among the additional music we’ll incorporate will be complete original published works of influential composers such as Marenzio and Monteverdi, and representative amounts by Josquin, Lassus, Palestrina, da Rore, etc. Through this resource, enriched with metadata, we could explore for the first time, for example, the networks of influence, distribution and fashion, and the effects on these of political, religious and social change over time, as represented in the output of the burgeoning 16th-century music publishing industry.
Full-text searching is an absolute necessity for ‘distant reading’, the basis of much digital humanities research. The current limited availability of digital scores means musicologists cannot use full-text searches into the music of the past. F-TEMPO enhances and extends our proof-of-concept interface, providing research-quality full-text search of material of great interest to historical musicologists, other scholars and general users. We plan a system closely analogous for music to Google Books, which extracts from scanned page-images their underlying textual content, which is then indexed and searched efficiently using the latest techniques of computer science.

Context
Music exists in a variety of forms on the web, typically as printed or manuscript documents or as audio files. Whatever the format, it cannot be searched directly, unlike text. It requires processing to extract ‘features’ which may then be investigated using the techniques of digital scholarship. Music Information Retrieval (MIR), emergent since 2000, has contributed to this, and some of its attention is devoted to the needs of musicologists and others wishing to engage with music at a deep level. However, the amount of data generated by feature-extraction from either audio analysis or digital scores means full-text, content-based searching of anything approaching a large-scale musical resource tends to be slow and inefficient.
Vast amounts of music, mostly audio tracks, are now available using services such as Spotify, iTunes or YouTube. MIR has mostly focused on audio material to make discovery and retrieval feasible from the Internet, with an emphasis from the music industry on the requirements of their paying customers.
Music in graphic form is also available online as PDF files rendering page-images of either original musical documents or modern, computer-generated music notation. The online service IMSLP [3] offers much of this, retrievable by metadata queries. Users range from professional and amateur musicians seeking performance materials to musicologists and academic teachers needing to consult original documents. Such resources are a surrogate for traditional paper-based books used in traditional musicology, but offer few advantages beyond convenience. They don’t give the facility of full-text search, unlike the text-based and numerical materials which are increasingly the subject of ‘distant reading’ investigations in the digital humanities.

For good-quality score images, there are Optical Music Recognition (OMR) programs which sometimes produce useful scores from printed music of simple texture; however, in general, OMR output contains errors due to misrecognised or missed symbols. The results often amount to musical gibberish, because of this ‘noise’. This severely limits the usefulness of OMR for creating large digital score collections from score images.
However, for many applications in musicology and for certain repertories, OMR can be used at scale to generate large quantities of data of great potential use. This motivated the Peachnote project [4], which subjected millions of page-images from IMSLP to OMR, providing ways for users to search for a phrase of music within the entire collection in fractions of a second. Peachnote’s online interface can show the varying occurrence of a given musical phrase over the 500-year chronological span of music history represented in IMSLP’s data. But its ‘catch-all’ nature, the amount of error present in the data and the variability of its coverage does not make it useful for serious research.
The OMR data on which Peachnote is based is full of errors; since the source materials were so diverse, this is hardly surprising, as OMR programs are designed to deal with modern high-quality printing. However, it established a basic principle, that certain types of user need may be satisfied using the methods of computer science on large collections containing noise. This is also the principle behind searches in Google Books, based on Optical Character Recognition (OCR). Another online resource following the same paradigm, this time based on images of Japanese woodblock prints, is Ukiyo-e Search [5], which permits instantaneous identification of even a poor-quality image of a print taken with a mobile phone.
The SIMSSA project [6] uses OMR and MIR to work towards a very large virtual and distributed collection of music accessible in the way we envisage for musicologists and all types of other musicians. As Associate Partners in SIMSSA our work should be seen as a contribution to this international effort.

Methodology
Our approach, inspired equally by Google Books, Ukiyo-e Search, Peachnote and SIMSSA, but limited to the large repertory of early printed music of the 16th and early 17th centuries, builds on our prototype search mechanism and interface [2] using state-of-the-art, scalable retrieval methods [7]. This currently provides rapid searches of 32,000 EMO page-images for those similar to a query-page in less than a second. It successfully recovers matches when the query page is not complete, e.g. when page-breaks occur differently in the various editions of a piece. Also, close non-identical matches, as between voice-parts of a polyphonic work in imitative style, are highly ranked in results; similarly, different works based on the same musical content (as in different sections) are usually well-matched.
Our OMR program is Aruspix [http://www.aruspix.net], highly reliable on images from EMO, digitised from microfilm. Most new images will be of higher quality; we are confident of useful results. We extract diatonic pitch-interval strings, robust to several types of OMR error involving wrong clefs, key-signatures and accidentals, for each page.
From these we derive sets of features recently developed for bioinformatics analysis and retrieval, Minimal Absent Words. We store these on a remote server in a trie (suffix-tree) data structure for fast and scalable search and retrieval via POST requests from a normal internet browser.
We shall expand and develop the collection without any new digitisation; the processes we adopt, existing and tested methods, are those of recognition, feature extraction and indexing – no digitisation as such is included in our plan, as ‘new’ digital images will be contributed by our consortium of music libraries. This expanding test collection (aiming eventually at a target of 1m) will be a freely available international resource for musicians of all kinds.
By providing from the outset an easy-to-use web interface and an easily-programmed API, we ensure musicologists can use the resource for any number of other purposes in the future. We shall investigate four basic research questions to demonstrate its potential power.
Research questions
1. ‘How did popularity change over time?’ As the virtual corpus spans the 16th and early 17th centuries, while musical fashions naturally evolved, we shall carry out empirical research on the enduring popularity of some works by early masters some of which survived for a whole century. Such investigations have only been carried out
‘manually’ by piecemeal research, and some ‘hits’ may have been unfairly overlooked.
2. ‘How well is the relative popularity of early-modern music reflected in modern recordings since the 1950s?’ We shall answer this question by using Linked Data to match works from the renaissance period with modern recordings (via MusicBrainz).
3. ‘How different is church music from madrigals?’ We observed in prototype testing that if a motet or mass movement is the query, high non-relevant matches tend also to be sacred pieces, and, madrigals, etc., return other secular music. This is a phenomenon of style that has not, as far as we know, been investigated at any scale.

4. ‘How many arrangements are there?’ In early testing we identified an instrumental ricercar as a wordless transcription of a Latin motet, hitherto unknown to musicology (see [7]). As the collection grows, we shall find more such unexpected concordances, and be able to identify works labelled in some printed sources as by ‘Incertus’ (Uncertain composer); some can already be identified within the EMO corpus so we know success is likely.

References
1. https://repository.royalholloway.ac.uk/access/hierarchy.do?topic=52facdbd- 19ce-2b92-dbd5-434289d29e8b&sort=rank&page=1
2. http://doc.gold.ac.uk/~mas01tc/emoji
3. http://imslp.org
4. http://www.peachnote.com
5. https://ukiyo-e.org
6. https://simssa.ca
7. http://doc.gold.ac.uk/~mas01tc/ISMIR2018_draft.pdf

We shall be collaborating closely with three principal organisations:
Providers of image material and conveners of consortium of donors: The British Library Music Department (Dr Amelie Roper)
The University of Tours, France (President, Prof Philippe Vendrix)
   
Technical advisor:
Répértoire Internationale des Sources Musicales, Swiss Office (Dr Laurent Pugin)

Other participants and advisors (unfunded) are likely to include Dr Jamie Forth, Dr Golnaz Badkobeh and Dr Christophe Rhodes (Goldsmiths, Computing department) who have been involved in the prototype development. Others include Mr David Lewis (Oxford & Goldsmiths) and Dr Kevin Page (Oxford)